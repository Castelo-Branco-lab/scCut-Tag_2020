
configfile: os.path.dirname(workflow.basedir) + '/config/config.yaml'
include: 'preflight'
include: 'snakefile_preprocess'

# Merge and run clustering together
rule clustering_all:
  input:
    expand("results/{antibody}/clustering/01.clustering.Rds",antibody = antibody_dict.keys()),
    expand("results/other_datasets/correlation/{antibody}/bed/", antibody = antibody_dict.keys()),
    expand("results/other_datasets/correlation/{antibody}/bam/", antibody = antibody_dict.keys()),
    expand("results/other_datasets/correlation/{antibody}/bigwig/", antibody = antibody_dict.keys()),


########### FINAL TWEAKED CLUSTERING
rule clustering_final:
  input:
    lambda wildcards: expand("results/{sample}/seurat/bin_5000/Seurat_object.Rds", sample = [x for x in samples_dict if samples_dict[x] == wildcards.antibody]),
    notebook       = os.path.dirname(workflow.basedir) + "/notebooks/{antibody}/{antibody}_clustering_merge.Rmd",
    RNAmarkers     = 'results/Sten_RNA/clustering/sten_RNA_markers.csv',
    GFPmarkers     = 'results/Sox10_RNA/clustering/GFP/markers.csv',
    fragments      = "results/{antibody}/fragments.tsv.gz",
    # peaks_narrow   = 'results/merged/{antibody}/macs/narrow/{antibody}_peaks.narrowPeak', # TODO: Fix atac conflict
    # peaks_broad    = 'results/merged/{antibody}/macs/broad/{antibody}_peaks.broadPeak' # TODO: Fix atac conflict
    # TODO: Olig2 depends on H3K27ac, write lambda function for that 
  output:
    seurat  = "results/{antibody}/clustering/01.clustering.Rds",
    report  = "results/{antibody}/clustering/01.clustering.html",
    markers = "results/{antibody}/clustering/markers.csv",
    bigwig  = directory("results/{antibody}/clustering/bigwig/"),
    bed     = directory("results/{antibody}/clustering/markers_bed/"),
    table   = "results/{antibody}/clustering/bam_per_cluster/cluster_barcode_table.csv",
    
  params:
    config         = config_file,
    out_prefix     = CWD + '/results/',
    report         = CWD + '/results/{antibody}/clustering/01.clustering.html',
  shell:
    " Rscript -e \"rmarkdown::render(input='{input.notebook}',output_file = '{params.report}', params=list(config='{params.config}',out_prefix = '{params.out_prefix}',antibody = '{wildcards.antibody}'))\" "


# Export bed per cluster
rule bed_per_cluster_and_replicate:
  input:
    fragments = "results/{antibody}/fragments.tsv.gz",
    barcodes  = "results/{antibody}/clustering/bam_per_cluster/cluster_barcode_table.csv"
  output:
    directory("results/other_datasets/correlation/{antibody}/bed/")
  params:
    script = os.path.dirname(workflow.basedir) + "/scripts/export_bam_per_replicate_and_cluster.py"
  shell:
    """
    python3 {params.script} -f {input.fragments}  -b {input.barcodes}  -o {output} 
    """

# bed per cluster to bam

rule bed_to_bam_per_cluster_and_replicate:
  input:
    bed = "results/other_datasets/correlation/{antibody}/bed/",
    genome = "results/general/mm10.chromSizes",
  output:
    directory("results/other_datasets/correlation/{antibody}/bam/")
  shell:
    """
    ls {input.bed}*.bed | while read line; do 
      BAM=$(basename ${{line/.bed/.bam}})
      bedToBam -i $line -g {input.genome} > {output}$BAM && samtools index {output}$BAM & 
    done
    wait $(jobs -p)
    """

# Bam per cluster to bw
rule bam_to_bw_bam_per_cluster:
  input:
    "results/other_datasets/correlation/{antibody}/bam/"
  output:
    directory("results/other_datasets/correlation/{antibody}/bigwig/")
  threads: 16
  shell:
    """
    ls {input}*.bam | while read line; do
      BIGWIG=$(basename ${{line/.bam/.bw}})
      bamCoverage -b $line -o {output}$BIGWIG --normalizeUsing RPKM -p {threads}
    done
    """
